title:
RuntimeError: Quantization not yet supported for op: Without OP specifier #52621

user:
lukqw 

status:
open
contributor:
gunan


labels:
comp:lite
stat:awaiting response
TF 2.3
TFLiteConverter
type:support

description:
1. System information
Windows 10
Tensorflow 2.3.0rc0 (also tried 2.3.1)

2. Code
I am trying to convert my custom yolov3-tiny (with relu instead of leakyRelu) SavedModel to 8bit quantized tflite for edge tpu support with the following:

import tensorflow as tf
import numpy as np

def representative_dataset_gen():
    for _ in range(250):
        yield [np.random.uniform(0.0, 1.0, size=(1, 416, 416, 3)).astype(np.float32)]

model = tf.keras.models.load_model('yolo_relu_model')

batch_size = 1
input_shape = model.inputs[0].shape.as_list()
input_shape[0] = batch_size
func = tf.function(model).get_concrete_function(tf.TensorSpec(input_shape, model.inputs[0].dtype))
converter = tf.lite.TFLiteConverter.from_concrete_functions([func])

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
converter.experimental_new_quantizer = True

model_lite = converter.convert()
f = open("yolo_relu_model.tflite", "wb")
f.write(model_lite)
f.close()
which leads to:

  File "quantize.py", line 37, in <module>
    quantize_model("yolo_relu")
  File "quantize.py", line 31, in quantize_model
    model_lite = converter.convert()
  File "C:\venv\lib\site-packages\tensorflow\lite\python\lite.py", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File "C:\venv\lib\site-packages\tensorflow\lite\python\lite.py", line 900, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File "C:\venv\lib\site-packages\tensorflow\lite\python\lite.py", line 638, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File "C:\venv\lib\site-packages\tensorflow\lite\python\lite.py", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File "C:\venv\lib\site-packages\tensorflow\lite\python\optimize\calibrator.py", line 98, in calibrate_and_quantize       
    np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Quantization not yet supported for op:
I have also started a colab here which runs on tensorflow 2.6 (I cant seem to change the version), which produces a different output.

you can download the zip of my model here and upload the zip inside colab.

In colab I get this output:

WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named "keras_metadata.pb" in the SavedModel directory.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.
WARNING:absl:For model outputs containing unsupported operations which cannot be quantized, the `inference_output_type` attribute will default to the original type.
When converting the tflite produced by colab with edgetpu_compiler I get the following result:

Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 1270 ms.

Input model: co_yolo_relu_model.tflite
Input size: 8.57MiB
Output model: co_yolo_relu_model_edgetpu.tflite
Output size: 8.61MiB
On-chip memory used for caching model parameters: 1.95MiB
On-chip memory remaining for caching model parameters: 15.75KiB
Off-chip memory used for streaming uncached model parameters: 6.54MiB
Number of Edge TPU subgraphs: 1
Total number of operations: 62
Operation log: co_yolo_relu_model_edgetpu.log

Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 22
Number of operations that will run on CPU: 40
See the operation log file for individual operation details.
Compilation child process completed within timeout period.
Compilation succeeded!
I'd like to have all operations on the Edge TPU (obviously), but I am kinda stumped here on how to proceed.

Can someone point me in the right direction on how to convert the model correctly (with tf 2.3.1 or others)?

Why is there no specifier on which op is not supported in the error message
RuntimeError: Quantization not yet supported for op:
?

This is the log of edgetpu_compiler:

Edge TPU Compiler version 16.0.384591198
Input: co_yolo_relu_model.tflite
Output: co_yolo_relu_model_edgetpu.tflite

Operator                       Count      Status

STRIDED_SLICE                  8          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
MUL                            6          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
DEQUANTIZE                     2          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
CONCATENATION                  4          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
CONCATENATION                  1          Mapped to Edge TPU
EXP                            2          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
CONV_2D                        13         Mapped to Edge TPU
QUANTIZE                       1          Mapped to Edge TPU
QUANTIZE                       4          More than one subgraph is not supported
QUANTIZE                       4          Operation is otherwise supported, but not mapped due to some unspecified limitation
RESIZE_NEAREST_NEIGHBOR        1          Mapped to Edge TPU
LOGISTIC                       6          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
MAX_POOL_2D                    6          Mapped to Edge TPU
RESHAPE                        2          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
ADD                            2          Tensor has unsupported rank (up to 3 innermost dimensions mapped)
